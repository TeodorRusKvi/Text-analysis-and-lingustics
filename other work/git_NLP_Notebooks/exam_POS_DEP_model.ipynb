{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout, concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (12854, 1)\n",
      "Shape of X: (12854, 20)\n",
      "Shape of embeddings_GloVe: (22235, 100)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset to see its structure\n",
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "df = pd.read_csv(url_data + 'new_df.csv')\n",
    "\n",
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_df = pd.read_csv(url_data+'X_tensorflow.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "X = X_df.to_numpy()\n",
    "\n",
    "X_txt = pd.read_csv(url_data + 'X_text.csv')\n",
    "X_txt = X_txt.to_numpy()\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y = y_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()\n",
    "\n",
    "print('Shape of label tensor:', y.shape)\n",
    "print('Shape of X:', X.shape)\n",
    "print('Shape of embeddings_GloVe:', embeddings_GloVe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing hyperparameters\n",
    "max_len = 20\n",
    "trunc_type = \"post\"\n",
    "padding_type = \"post\"\n",
    "vocab_size = len(embeddings_GloVe)\n",
    "# This is fixed.\n",
    "embedding_dim = 100\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_tags = df['POS_Tags'].to_list()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "POS_tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the POS tags\n",
    "POS_tokenizer.fit_on_texts(POS_tags)\n",
    "\n",
    "# Convert POS tags to sequences\n",
    "POS_sequences = POS_tokenizer.texts_to_sequences(POS_tags)\n",
    "\n",
    "# Pad the sequences\n",
    "POS_padded= pad_sequences(POS_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "POS_dict = POS_tokenizer.word_index\n",
    "POS_size = len(POS_dict)\n",
    "POS_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_tags = df['Dependency_Tags'].to_list()\n",
    "\n",
    "Dep_tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the POS tags\n",
    "Dep_tokenizer.fit_on_texts(dep_tags)\n",
    "\n",
    "# Convert POS tags to sequences\n",
    "Dep_sequences = Dep_tokenizer.texts_to_sequences(dep_tags)\n",
    "\n",
    "# Pad the sequences\n",
    "Dep_padded= pad_sequences(Dep_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "Dep_dict = Dep_tokenizer.word_index\n",
    "Dep_size = len(Dep_dict)\n",
    "Dep_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dep_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text, X_test_text, POS_train, POS_test, Dep_train, Dep_test, y_train, y_test = train_test_split(X, POS_padded, Dep_padded, y, test_size=0.7, random_state=42)\n",
    "X_val_text, X_test_text, POS_val, POS_test, Dep_val, Dep_test, y_val, y_test = train_test_split(X_test_text, POS_test, Dep_test, y_test, test_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " text_input (InputLayer)     [(None, 20)]                 0         []                            \n",
      "                                                                                                  \n",
      " pos_input (InputLayer)      [(None, 20)]                 0         []                            \n",
      "                                                                                                  \n",
      " dep_input (InputLayer)      [(None, 20)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding_15 (Embedding)    (None, 20, 100)              2223500   ['text_input[0][0]']          \n",
      "                                                                                                  \n",
      " embedding_16 (Embedding)    (None, 20, 10)               180       ['pos_input[0][0]']           \n",
      "                                                                                                  \n",
      " embedding_17 (Embedding)    (None, 20, 10)               430       ['dep_input[0][0]']           \n",
      "                                                                                                  \n",
      " spatial_dropout1d_15 (Spat  (None, 20, 100)              0         ['embedding_15[0][0]']        \n",
      " ialDropout1D)                                                                                    \n",
      "                                                                                                  \n",
      " spatial_dropout1d_16 (Spat  (None, 20, 10)               0         ['embedding_16[0][0]']        \n",
      " ialDropout1D)                                                                                    \n",
      "                                                                                                  \n",
      " spatial_dropout1d_17 (Spat  (None, 20, 10)               0         ['embedding_17[0][0]']        \n",
      " ialDropout1D)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)          (None, 20, 60)               6060      ['spatial_dropout1d_15[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)          (None, 20, 20)               220       ['spatial_dropout1d_16[0][0]']\n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)          (None, 20, 20)               220       ['spatial_dropout1d_17[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling1d_15 (MaxPooli  (None, 10, 60)               0         ['conv1d_15[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " max_pooling1d_16 (MaxPooli  (None, 10, 20)               0         ['conv1d_16[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " max_pooling1d_17 (MaxPooli  (None, 10, 20)               0         ['conv1d_17[0][0]']           \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " dense_25 (Dense)            (None, 10, 100)              6100      ['max_pooling1d_15[0][0]']    \n",
      "                                                                                                  \n",
      " dense_26 (Dense)            (None, 10, 50)               1050      ['max_pooling1d_16[0][0]']    \n",
      "                                                                                                  \n",
      " dense_27 (Dense)            (None, 10, 50)               1050      ['max_pooling1d_17[0][0]']    \n",
      "                                                                                                  \n",
      " bidirectional_15 (Bidirect  (None, 10, 128)              84480     ['dense_25[0][0]']            \n",
      " ional)                                                                                           \n",
      "                                                                                                  \n",
      " bidirectional_16 (Bidirect  (None, 10, 40)               11360     ['dense_26[0][0]']            \n",
      " ional)                                                                                           \n",
      "                                                                                                  \n",
      " bidirectional_17 (Bidirect  (None, 10, 40)               11360     ['dense_27[0][0]']            \n",
      " ional)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate  (None, 10, 208)              0         ['bidirectional_15[0][0]',    \n",
      " )                                                                   'bidirectional_16[0][0]',    \n",
      "                                                                     'bidirectional_17[0][0]']    \n",
      "                                                                                                  \n",
      " additive_attention_5 (Addi  (None, 10, 208)              0         ['concatenate_5[0][0]',       \n",
      " tiveAttention)                                                      'concatenate_5[0][0]']       \n",
      "                                                                                                  \n",
      " global_average_pooling1d_5  (None, 208)                  0         ['additive_attention_5[0][0]']\n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " dense_28 (Dense)            (None, 100)                  20900     ['global_average_pooling1d_5[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 100)                  0         ['dense_28[0][0]']            \n",
      "                                                                                                  \n",
      " dense_29 (Dense)            (None, 1)                    101       ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2367011 (9.03 MB)\n",
      "Trainable params: 143511 (560.59 KB)\n",
      "Non-trainable params: 2223500 (8.48 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, Conv1D, MaxPooling1D, Dense, Bidirectional, LSTM, AdditiveAttention, GlobalAveragePooling1D, Dropout, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Text pathway\n",
    "text_input = Input(shape=(max_len,), dtype='int32', name='text_input')\n",
    "text_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embeddings_GloVe], trainable=False)(text_input)\n",
    "text_spatial_dropout = SpatialDropout1D(0.2)(text_embedding)\n",
    "text_conv1d = Conv1D(filters=60, kernel_size=1, activation='relu')(text_spatial_dropout)\n",
    "text_max_pooling = MaxPooling1D()(text_conv1d)\n",
    "text_conv_dense = Dense(100, activation='relu')(text_max_pooling)\n",
    "text_lstm = Bidirectional(LSTM(64, activation='tanh', recurrent_activation='sigmoid', recurrent_dropout=0.1, return_sequences=True))(text_conv_dense)\n",
    "\n",
    "# POS pathway\n",
    "pos_input = Input(shape=(max_len,), dtype='int32', name='pos_input')\n",
    "pos_embedding = Embedding(input_dim=POS_size+1, output_dim=10)(pos_input)\n",
    "pos_spatial_dropout = SpatialDropout1D(0.2)(pos_embedding)\n",
    "pos_conv1d = Conv1D(filters=20, kernel_size=1, activation='relu')(pos_spatial_dropout)\n",
    "pos_max_pooling = MaxPooling1D()(pos_conv1d)\n",
    "pos_conv_dense = Dense(50, activation='relu')(pos_max_pooling)\n",
    "pos_lstm = Bidirectional(LSTM(20, activation='tanh', recurrent_activation='sigmoid', recurrent_dropout=0.1, return_sequences=True))(pos_conv_dense)\n",
    "\n",
    "# Dep pathway\n",
    "dep_input = Input(shape=(max_len,), dtype='int32', name='dep_input')\n",
    "dep_embedding = Embedding(input_dim=Dep_size+1, output_dim=10)(dep_input)\n",
    "dep_spatial_dropout = SpatialDropout1D(0.2)(dep_embedding)\n",
    "dep_conv1d = Conv1D(filters=20, kernel_size=1, activation='relu')(dep_spatial_dropout)\n",
    "dep_max_pooling = MaxPooling1D()(dep_conv1d)\n",
    "dep_conv_dense = Dense(50, activation='relu')(dep_max_pooling)\n",
    "dep_lstm = Bidirectional(LSTM(20, activation='tanh', recurrent_activation='sigmoid', recurrent_dropout=0.1, return_sequences=True))(dep_conv_dense)\n",
    "\n",
    "# Combine pathways\n",
    "combined = concatenate([text_lstm, pos_lstm, dep_lstm])\n",
    "\n",
    "# Attention\n",
    "attention_layer = AdditiveAttention(use_scale=False)\n",
    "attention_output = attention_layer([combined, combined], return_attention_scores=False)\n",
    "attention_output = GlobalAveragePooling1D()(attention_output)\n",
    "\n",
    "# Final dense layers\n",
    "dense_relu = Dense(100, activation='relu')(attention_output)\n",
    "dropout = Dropout(0.20)(dense_relu)\n",
    "output_layer = Dense(num_classes, activation='sigmoid')(dropout)\n",
    "\n",
    "# Create and compile model\n",
    "model = Model(inputs=[text_input, pos_input, dep_input], outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model_build(vocab_size=vocab_size, max_len=max_len, embedding_dim=embedding_dim, embeddings=embeddings_GloVe, \n",
    "                    # POS_size=POS_size, Dep_size=Dep_size, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "121/121 [==============================] - 70s 122ms/step - loss: 0.6530 - accuracy: 0.6362 - val_loss: 0.6182 - val_accuracy: 0.6525\n",
      "Epoch 2/20\n",
      "121/121 [==============================] - 6s 48ms/step - loss: 0.6079 - accuracy: 0.6642 - val_loss: 0.5871 - val_accuracy: 0.6814\n",
      "Epoch 3/20\n",
      "121/121 [==============================] - 6s 47ms/step - loss: 0.5780 - accuracy: 0.6981 - val_loss: 0.5800 - val_accuracy: 0.6866\n",
      "Epoch 4/20\n",
      "121/121 [==============================] - 6s 49ms/step - loss: 0.5630 - accuracy: 0.7108 - val_loss: 0.5554 - val_accuracy: 0.7180\n",
      "Epoch 5/20\n",
      "121/121 [==============================] - 6s 46ms/step - loss: 0.5370 - accuracy: 0.7290 - val_loss: 0.5628 - val_accuracy: 0.7092\n",
      "Epoch 6/20\n",
      "121/121 [==============================] - 6s 48ms/step - loss: 0.5122 - accuracy: 0.7490 - val_loss: 0.5581 - val_accuracy: 0.7199\n",
      "Epoch 7/20\n",
      "121/121 [==============================] - 6s 47ms/step - loss: 0.4854 - accuracy: 0.7609 - val_loss: 0.5710 - val_accuracy: 0.7214\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)\n",
    "\n",
    "\n",
    "history = model.fit([X_train_text, POS_train, Dep_train], y_train,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=([X_val_text, POS_val, Dep_val], y_val),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 2s 11ms/step - loss: 0.5533 - accuracy: 0.7315\n",
      "Test Loss: 0.5533388257026672\n",
      "Test Accuracy: 0.7315446734428406\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate([X_test_text, POS_test, Dep_test], y_test, batch_size=BATCH_SIZE)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
