{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "# import wandb\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, LSTM, Dropout, Bidirectional, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Concatenate, BatchNormalization, MultiHeadAttention, LayerNormalization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y = y_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()\n",
    "\n",
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_df = pd.read_csv(url_data+'new_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_df.columns:\n",
    "    X_df[col] = X_df[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1= X_df['with_stopwords']\n",
    "\n",
    "# Konverter kolonnen til et NumPy array\n",
    "X = X_1.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    12854.000000\n",
       "mean        47.516104\n",
       "std        182.683952\n",
       "min          1.000000\n",
       "25%         10.000000\n",
       "50%         13.000000\n",
       "75%         23.000000\n",
       "max       5634.000000\n",
       "Name: without_stopwords, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts_length = X_df['without_stopwords'].apply(lambda x: len(x.split()))\n",
    "# Now, let's analyze the distribution of these sequence lengths\n",
    "all_texts_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, min_df=0.01, stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Get feature names to later identify words by index\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate the average TF-IDF score for each word across all documents\n",
    "avg_scores = np.mean(X_tfidf.toarray(), axis=0)\n",
    "\n",
    "# Set a threshold, for example, the mean of average scores\n",
    "threshold = np.mean(avg_scores)\n",
    "\n",
    "# Filter words that have a score above the threshold\n",
    "important_words = [feature_names[i] for i in range(len(feature_names)) if avg_scores[i] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter corpus to keep only important words\n",
    "filtered_texts = []\n",
    "for doc in X_1:\n",
    "    tokens = doc.split()\n",
    "    filtered_tokens = [token for token in tokens if token in important_words]\n",
    "    filtered_texts.append(' '.join(filtered_tokens))\n",
    "\n",
    "# Tokenize texts\n",
    "tokenizer = Tokenizer(oov_token='OOV')\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_seq_length = max(len(seq) for seq in sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22333 unique tokens.\n",
      "\n",
      "First 10 is listen below:\n",
      "{'OOV': 1, 'nan': 2, 'people': 3, 'like': 4, 'work': 5, 'right': 6, 'trump': 7, 'think': 8, 'state': 9, 'government': 10}\n"
     ]
    }
   ],
   "source": [
    "#Creating a word index of the words from the tokenizer \n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.\\n\\nFirst 10 is listen below:')\n",
    "print(dict(list(word_index.items())[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing hyperparameters for the networks\n",
    "max_len = 100\n",
    "trunc_type = \"post\"\n",
    "padding_type = \"post\"\n",
    "vocab_size = len(word_index)\n",
    "# This is fixed.\n",
    "embedding_dim = 100\n",
    "EPOCHS=20\n",
    "BATCH_SIZE = 32\n",
    "num_classes = 1\n",
    "\n",
    "# Padding the sequences to keep the lengths uniform\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "# print('Shape of data tensor:', X_tensorflow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42, stratify=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self, max_len, num_classes, embeddings_GloVe):\n",
    "        self.max_len = max_len\n",
    "        self.num_classes = num_classes\n",
    "        self.embeddings_GloVe = embeddings_GloVe\n",
    "\n",
    "\n",
    "# Create a configuration object\n",
    "config = ModelConfig(max_len=max_len, num_classes=num_classes, embeddings_GloVe=embeddings_GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_LSTM_sequential(params, config):\n",
    "    input_layer = Input(shape=(config.max_len,), dtype='int32')\n",
    "    \n",
    "    # Use config object for fixed parameters such as embeddings\n",
    "    embedding = Embedding(input_dim=config.embeddings_GloVe.shape[0],\n",
    "                          output_dim=config.embeddings_GloVe.shape[1],\n",
    "                        #   weights=[config.embeddings_GloVe],\n",
    "                          trainable=True)(input_layer)\n",
    "    \n",
    "    # Use params dictionary for dynamic hyperparameters\n",
    "    dropout = Dropout(params['dropout_rate'])(embedding)\n",
    "\n",
    "    conv = Conv1D(filters=params['conv_filters'], kernel_size=1, activation='relu')(dropout)\n",
    "    conv = BatchNormalization()(conv)\n",
    "\n",
    "    lstm = Bidirectional(LSTM(params['lstm_units'], return_sequences=True, dropout=0.006, recurrent_dropout=0.1))(conv)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    \n",
    "    num_heads = 8\n",
    "    attention_layer = MultiHeadAttention(num_heads=num_heads, key_dim=config.embeddings_GloVe.shape[1] // num_heads, dropout=0.1)\n",
    "    attention_output = attention_layer(query=lstm, key=lstm, value=lstm)\n",
    "    attention_output = LayerNormalization()(attention_output)\n",
    "\n",
    "    dense = Dense(params['dense_2_units'], activation='relu')(attention_output)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    output = Dense(config.num_classes, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(params['learning_rate']), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import optuna\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from optuna.integration import KerasPruningCallback\n",
    "import optuna_dashboard\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the objective function including the logging of each trial's outcome\n",
    "# def objective(trial):\n",
    "#     # wandb.init(project=\"optuna_with_wandb\", entity=\"your_entity_here\", reinit=True)\n",
    "#     params = {\n",
    "#         'lstm_units': trial.suggest_int('lstm_units', 135, 145, step=1),\n",
    "#         # 'dense_1_units': trial.suggest_int('dense_1_units', 100, 150, step=5),\n",
    "#         'dense_2_units': trial.suggest_int('dense_2_units', 140, 160, step=2),\n",
    "#         'dropout_rate': trial.suggest_float('dropout_rate', 0.3, 0.5),\n",
    "#         # 'lstm_dropout': trial.suggest_float('lstm_dropout', 0.0, 0.2),\n",
    "#         # 'lstm_recurrent': trial.suggest_float('lstm_recurrent', 0.0, 0.2),\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "#         'conv_filters': trial.suggest_int('conv_filters', 46, 60, step=2)\n",
    "#     }\n",
    "\n",
    "#     # # Log hyperparameters to wandb\n",
    "#     # wandb.config.update(params)\n",
    "\n",
    "#     # Build and train the model\n",
    "#     model = CNN_LSTM_sequential(params, config)\n",
    "\n",
    "#     #Callbacks\n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "#     pruning_callback = KerasPruningCallback(trial, 'val_loss')  # Create a pruning callback\n",
    "#     # Fit the model\n",
    "#     history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10,\n",
    "#                         callbacks=[early_stopping, pruning_callback], batch_size=32, verbose=1)\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     loss, accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
    "    \n",
    "#     # Log final metrics to wandb\n",
    "#     # wandb.log({\"loss\": loss, \"accuracy\": accuracy})\n",
    "\n",
    "#     # Ensure wandb run is finished after each trial\n",
    "#     # wandb.finish()\n",
    "    \n",
    "#     # trial.report(accuracy, 1)  # Report the accuracy to Optuna\n",
    "#     return accuracy \n",
    "\n",
    "\n",
    "# # Setup Optuna with persistent storage\n",
    "# storage_url = \"sqlite:///db.sqlite3\"\n",
    "# study_name = 'Proto_koer'\n",
    "# # optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "# study = optuna.create_study(direction='maximize', \n",
    "#                             sampler=optuna.samplers.TPESampler(), \n",
    "#                             study_name=study_name, \n",
    "#                             storage=storage_url, \n",
    "#                             pruner=optuna.pruners.MedianPruner(),  # Adding a pruner\n",
    "#                             load_if_exists=True)\n",
    "\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(f\"Best value: {study.best_value} (params: {study.best_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data to be saved as JSON\n",
    "# data = {\n",
    "#     \"params\": {\n",
    "#         \"lstm_units\": 135,\n",
    "#         \"dense_2_units\": 160,\n",
    "#         \"dropout_rate\": 0.43832284397692234,\n",
    "#         \"learning_rate\": 0.0004893640558066397,\n",
    "#         \"conv_filters\": 56\n",
    "#     },\n",
    "#     \"best_accuracy\": 0.73934644460678\n",
    "# }\n",
    "\n",
    "# # File path for JSON file\n",
    "# file_path = 'best_trial_tf_idf.json'\n",
    "\n",
    "# # Saving data as JSON\n",
    "# with open(file_path, 'w') as file:\n",
    "#     json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the params from CNN-LSTM from the saved JSON-fil\n",
    "with open(r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\git_NLP_Notebooks\\best_trial_tf_idf.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    params = data['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/282 [..............................] - ETA: 1:16 - loss: 1.4209 - accuracy: 0.4859 "
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node model_4/embedding_4/embedding_lookup defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n\n  File \"C:\\Users\\bugat\\AppData\\Local\\Temp\\ipykernel_38848\\2982834495.py\", line 3, in <module>\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 590, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 515, in call\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 672, in _run_internal_graph\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 272, in call\n\nindices[9,0] = 22322 is not in [0, 22235)\n\t [[{{node model_4/embedding_4/embedding_lookup}}]] [Op:__inference_train_function_26503]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m best_model \u001b[38;5;241m=\u001b[39m CNN_LSTM_sequential(params, config)\n\u001b[1;32m----> 3\u001b[0m best_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Retrain on the full training data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Evaluate on the test data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m loss_1, accuracy_1 \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node model_4/embedding_4/embedding_lookup defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n\n  File \"C:\\Users\\bugat\\AppData\\Local\\Temp\\ipykernel_38848\\2982834495.py\", line 3, in <module>\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 590, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 515, in call\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 672, in _run_internal_graph\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 272, in call\n\nindices[9,0] = 22322 is not in [0, 22235)\n\t [[{{node model_4/embedding_4/embedding_lookup}}]] [Op:__inference_train_function_26503]"
     ]
    }
   ],
   "source": [
    "best_model = CNN_LSTM_sequential(params, config)\n",
    "\n",
    "best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, verbose=1)\n",
    "# Retrain on the full training data\n",
    "\n",
    "# Evaluate on the test data\n",
    "loss_1, accuracy_1 = best_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Accuracy: {accuracy_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_acc}, Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "plt.title('Accuracy score')\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.show()\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.title('Loss value')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
