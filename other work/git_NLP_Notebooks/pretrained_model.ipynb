{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "df = pd.read_csv(url_data + 'new_df.csv')\n",
    "y_liberal = pd.read_csv(url_data + 'y_liberal.csv')\n",
    "y_data = pd.read_csv(url_data + 'y_data.csv')\n",
    "X_text = pd.read_csv(url_data + 'X_text.csv')\n",
    "X_text_list= X_text['Processed'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|â–Œ         | 100/1607 [1:25:03<35:49:45, 85.59s/it] "
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Combine X_text and y_data for processing\n",
    "combined_df = pd.concat([X_text, y_data], axis=1)\n",
    "\n",
    "results_list = []\n",
    "\n",
    "# Process batches of data\n",
    "batch_size = 8  # Adjust based on your GPU memory\n",
    "for i in tqdm(range(0, len(combined_df), batch_size), desc=\"Processing batches\"):\n",
    "    batch = combined_df.iloc[i:i+batch_size]\n",
    "    texts = batch['Processed'].tolist()\n",
    "\n",
    "    # Prepare hypotheses for all labels and texts\n",
    "    labels = ['Conservative', 'Liberal']\n",
    "    batch_results = []\n",
    "    for label in labels:\n",
    "        hypotheses = [f\"This example is {label}.\" for _ in texts]\n",
    "        inputs = tokenizer(texts, hypotheses, return_tensors='pt', padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = logits.softmax(dim=1)\n",
    "            entailment_probs = probs[:, 2]  # Index 2 corresponds to \"entailment\"\n",
    "\n",
    "        # Store probabilities with texts\n",
    "        for prob, text in zip(entailment_probs, texts):\n",
    "            batch_results.append((text, label, prob.item()))\n",
    "\n",
    "    # Find the highest probability for each text in the batch\n",
    "    for j in range(len(texts)):\n",
    "        text_results = batch_results[j*len(labels):(j+1)*len(labels)]\n",
    "        max_result = max(text_results, key=lambda x: x[2])\n",
    "        results_list.append({\n",
    "            'text': max_result[0],\n",
    "            'label': max_result[1],\n",
    "            'probability': max_result[2]\n",
    "        })\n",
    "\n",
    "# Save results to a CSV file\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.to_csv(r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\git_NLP_data\\Bart_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
