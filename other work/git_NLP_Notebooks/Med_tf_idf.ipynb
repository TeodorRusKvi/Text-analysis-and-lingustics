{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "# import wandb\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, LSTM, Dropout, Bidirectional, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Concatenate, BatchNormalization, MultiHeadAttention, LayerNormalization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y = y_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()\n",
    "\n",
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_df = pd.read_csv(url_data+'new_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X_df['without_stopwords'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= X_df['without_stopwords'].to_numpy().flatten().astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_texts_length \u001b[38;5;241m=\u001b[39m X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwithout_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit()))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Now, let's analyze the distribution of these sequence lengths\u001b[39;00m\n\u001b[0;32m      3\u001b[0m all_texts_length\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4759\u001b[0m         func,\n\u001b[0;32m   4760\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4761\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4762\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4763\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4764\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1291\u001b[0m )\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_texts_length \u001b[38;5;241m=\u001b[39m X_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwithout_stopwords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit()))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Now, let's analyze the distribution of these sequence lengths\u001b[39;00m\n\u001b[0;32m      3\u001b[0m all_texts_length\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "all_texts_length = X_df['without_stopwords'].apply(lambda x: len(x.split()))\n",
    "# Now, let's analyze the distribution of these sequence lengths\n",
    "all_texts_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Get feature names to later identify words by index\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate the average TF-IDF score for each word across all documents\n",
    "avg_scores = np.mean(X.toarray(), axis=0)\n",
    "\n",
    "# Set a threshold, for example, the mean of average scores\n",
    "threshold = np.mean(avg_scores)\n",
    "\n",
    "# Filter words that have a score above the threshold\n",
    "important_words = [feature_names[i] for i in range(len(feature_names)) if avg_scores[i] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter corpus to keep only important words\n",
    "filtered_texts = []\n",
    "for doc in X_1:\n",
    "    tokens = doc.split()\n",
    "    filtered_tokens = [token for token in tokens if token in important_words]\n",
    "    filtered_texts.append(' '.join(filtered_tokens))\n",
    "\n",
    "# Tokenize texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(filtered_texts)\n",
    "sequences = tokenizer.texts_to_sequences(filtered_texts)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_seq_length = max(len(seq) for seq in sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3325 unique tokens.\n",
      "\n",
      "First 10 is listen below:\n",
      "{'the': 1, 'be': 2, 'to': 3, 'of': 4, 'and': 5, 'in': 6, 'nan': 7, 'that': 8, 'for': 9, 'it': 10}\n"
     ]
    }
   ],
   "source": [
    "#Creating a word index of the words from the tokenizer \n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.\\n\\nFirst 10 is listen below:')\n",
    "print(dict(list(word_index.items())[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing hyperparameters for the networks\n",
    "max_len = 100\n",
    "trunc_type = \"post\"\n",
    "padding_type = \"post\"\n",
    "vocab_size = len(word_index)\n",
    "# This is fixed.\n",
    "embedding_dim = 100\n",
    "EPOCHS=20\n",
    "BATCH_SIZE = 32\n",
    "num_classes = 1\n",
    "\n",
    "# Padding the sequences to keep the lengths uniform\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "# print('Shape of data tensor:', X_tensorflow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42, stratify=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self, max_len, num_classes, embeddings_GloVe):\n",
    "        self.max_len = max_len\n",
    "        self.num_classes = num_classes\n",
    "        self.embeddings_GloVe = embeddings_GloVe\n",
    "\n",
    "\n",
    "# Create a configuration object\n",
    "config = ModelConfig(max_len=max_len, num_classes=num_classes, embeddings_GloVe=embeddings_GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_LSTM_sequential(params, config):\n",
    "    input_layer = Input(shape=(config.max_len,), dtype='int32')\n",
    "    \n",
    "    # Use config object for fixed parameters such as embeddings\n",
    "    embedding = Embedding(input_dim=config.embeddings_GloVe.shape[0],\n",
    "                          output_dim=config.embeddings_GloVe.shape[1],\n",
    "                          weights=[config.embeddings_GloVe],\n",
    "                          trainable=False)(input_layer)\n",
    "    \n",
    "    # Use params dictionary for dynamic hyperparameters\n",
    "    dropout = Dropout(params['dropout_rate'])(embedding)\n",
    "\n",
    "    conv = Conv1D(filters=params['conv_filters'], kernel_size=1, activation='relu')(dropout)\n",
    "    conv = BatchNormalization()(conv)\n",
    "\n",
    "    lstm = Bidirectional(LSTM(params['lstm_units'], return_sequences=True, dropout=0.006, recurrent_dropout=0.1))(conv)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    \n",
    "    num_heads = 8\n",
    "    attention_layer = MultiHeadAttention(num_heads=num_heads, key_dim=config.embeddings_GloVe.shape[1] // num_heads, dropout=0.1)\n",
    "    attention_output = attention_layer(query=lstm, key=lstm, value=lstm)\n",
    "    attention_output = LayerNormalization()(attention_output)\n",
    "\n",
    "    dense = Dense(params['dense_2_units'], activation='relu')(attention_output)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    output = Dense(config.num_classes, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(params['learning_rate']), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the params from CNN-LSTM from the saved JSON-fil\n",
    "with open(r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\git_NLP_Notebooks\\best_trial_length_100.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    params = data['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lstm_units': 140,\n",
       " 'dense_2_units': 150,\n",
       " 'dropout_rate': 0.42803898610506674,\n",
       " 'learning_rate': 0.000922823163674921,\n",
       " 'conv_filters': 52}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\layer_normalization.py:328: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "282/282 [==============================] - 120s 389ms/step - loss: 0.6583 - accuracy: 0.6364 - val_loss: 0.6858 - val_accuracy: 0.6473\n",
      "Epoch 2/20\n",
      "282/282 [==============================] - 107s 380ms/step - loss: 0.6372 - accuracy: 0.6472 - val_loss: 0.6252 - val_accuracy: 0.6592\n",
      "Epoch 3/20\n",
      "282/282 [==============================] - 108s 382ms/step - loss: 0.6260 - accuracy: 0.6591 - val_loss: 0.6115 - val_accuracy: 0.6714\n",
      "Epoch 4/20\n",
      "282/282 [==============================] - 110s 389ms/step - loss: 0.6182 - accuracy: 0.6640 - val_loss: 0.6878 - val_accuracy: 0.6539\n",
      "Epoch 5/20\n",
      "282/282 [==============================] - 112s 399ms/step - loss: 0.6046 - accuracy: 0.6822 - val_loss: 0.5845 - val_accuracy: 0.7099\n",
      "Epoch 6/20\n",
      "282/282 [==============================] - 107s 381ms/step - loss: 0.5949 - accuracy: 0.6871 - val_loss: 0.5831 - val_accuracy: 0.7037\n",
      "Epoch 7/20\n",
      "282/282 [==============================] - 108s 382ms/step - loss: 0.5889 - accuracy: 0.6938 - val_loss: 0.5715 - val_accuracy: 0.7232\n",
      "Epoch 8/20\n",
      "282/282 [==============================] - 107s 378ms/step - loss: 0.5809 - accuracy: 0.7014 - val_loss: 0.5626 - val_accuracy: 0.7297\n",
      "Epoch 9/20\n",
      "282/282 [==============================] - 107s 381ms/step - loss: 0.5736 - accuracy: 0.7070 - val_loss: 0.5770 - val_accuracy: 0.7207\n",
      "Epoch 10/20\n",
      "282/282 [==============================] - 108s 384ms/step - loss: 0.5656 - accuracy: 0.7142 - val_loss: 0.5901 - val_accuracy: 0.7154\n",
      "Epoch 11/20\n",
      "282/282 [==============================] - 108s 384ms/step - loss: 0.5572 - accuracy: 0.7175 - val_loss: 0.5433 - val_accuracy: 0.7406\n",
      "Epoch 12/20\n",
      "282/282 [==============================] - 108s 384ms/step - loss: 0.5530 - accuracy: 0.7253 - val_loss: 0.5754 - val_accuracy: 0.7027\n",
      "Epoch 13/20\n",
      "282/282 [==============================] - 107s 380ms/step - loss: 0.5478 - accuracy: 0.7269 - val_loss: 0.5610 - val_accuracy: 0.7427\n",
      "Epoch 14/20\n",
      "282/282 [==============================] - 107s 379ms/step - loss: 0.5408 - accuracy: 0.7293 - val_loss: 0.5526 - val_accuracy: 0.7233\n",
      "Epoch 15/20\n",
      "282/282 [==============================] - 107s 378ms/step - loss: 0.5392 - accuracy: 0.7348 - val_loss: 0.5475 - val_accuracy: 0.7394\n",
      "Epoch 16/20\n",
      "282/282 [==============================] - 108s 382ms/step - loss: 0.5365 - accuracy: 0.7347 - val_loss: 0.5916 - val_accuracy: 0.7329\n",
      "Epoch 17/20\n",
      "282/282 [==============================] - 108s 384ms/step - loss: 0.5254 - accuracy: 0.7424 - val_loss: 0.5548 - val_accuracy: 0.7249\n",
      "Epoch 18/20\n",
      "282/282 [==============================] - 107s 381ms/step - loss: 0.5222 - accuracy: 0.7494 - val_loss: 0.5392 - val_accuracy: 0.7400\n",
      "Epoch 19/20\n",
      "282/282 [==============================] - 109s 386ms/step - loss: 0.5110 - accuracy: 0.7570 - val_loss: 0.5448 - val_accuracy: 0.7440\n",
      "Epoch 20/20\n",
      "282/282 [==============================] - 110s 390ms/step - loss: 0.5198 - accuracy: 0.7464 - val_loss: 0.5258 - val_accuracy: 0.7499\n",
      "61/61 [==============================] - 5s 82ms/step - loss: 0.5343 - accuracy: 0.7433\n",
      "Test Accuracy: 0.7432762980461121\n"
     ]
    }
   ],
   "source": [
    "best_model = CNN_LSTM_sequential(params, config)\n",
    "\n",
    "best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, verbose=1)\n",
    "# Retrain on the full training data\n",
    "\n",
    "# Evaluate on the test data\n",
    "loss_1, accuracy_1 = best_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Accuracy: {accuracy_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_acc}, Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "plt.title('Accuracy score')\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.show()\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.title('Loss value')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
