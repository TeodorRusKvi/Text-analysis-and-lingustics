{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_nlp.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras_nlp.models'"
     ]
    }
   ],
   "source": [
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras_nlp' has no attribute 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m keras_nlp\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mBertClassifier\u001b[38;5;241m.\u001b[39mfrom_preset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_tiny_en_uncased_sst2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras_nlp' has no attribute 'models'"
     ]
    }
   ],
   "source": [
    "classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TFDistilBertTokenizer' from 'transformers' (c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (TFBertForSequenceClassification, BertTokenizer)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFDistilBertTokenizer, TFDistilBertForSequenceClassification\n\u001b[0;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TFDistilBertTokenizer' from 'transformers' (c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import (TFBertForSequenceClassification, BertTokenizer)\n",
    "from transformers import TFDistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "max_len = 20\n",
    "\n",
    "bert_model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "bert_tokenizer = TFDistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "df = pd.read_csv(url_data + 'new_df.csv')\n",
    "\n",
    "df['All_text'] = df['All_text'].replace(['U.S.', 'U.S.A.'], ['US', 'USA'], regex=True)\n",
    "df['Processed'] = df['Processed'].replace(['U.S.', 'U.S.A.'], ['US', 'USA'], regex=True)\n",
    "df['Processed'] = df['Processed'].fillna(0)\n",
    "df['Processed'] = df['Processed'].astype(str)\n",
    "df['All_text'] = df['All_text'].fillna(0)\n",
    "df['All_text'] = df['All_text'].astype(str)\n",
    "\n",
    "# df.to_csv('new_df.csv', index=False)\n",
    "\n",
    "# Making the relevant columns to lists\n",
    "all_texts = (df['All_text'].to_list())\n",
    "texts = df['Processed'].to_list()\n",
    "\n",
    "# Setting the wanted text for further modelling\n",
    "corpus = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_train_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y_train = y_train_df.to_numpy()\n",
    "y = y_train.flatten().astype(str).tolist()\n",
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_nlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertClassifier\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define the number of classes based on y_train\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras_nlp'"
     ]
    }
   ],
   "source": [
    "from keras_nlp.models import BertClassifier\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the number of classes based on y_train\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "# Load a BERT model as the backbone\n",
    "# Here, 'bert-base-uncased' is a placeholder and typically requires loading a pre-trained model.\n",
    "# In this environment, we will assume such a model can be loaded.\n",
    "backbone = 'bert-base-uncased'  # This would normally be a model loaded from a pre-trained source.\n",
    "\n",
    "# Initialize the BERT Classifier\n",
    "model = BertClassifier(\n",
    "    backbone=backbone,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.1  # Set dropout rate\n",
    ")\n",
    "\n",
    "# Compile the model with an optimizer and a loss function appropriate for classification\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=3e-5),\n",
    "    loss='categorical_crossentropy',  # Suitable for multi-class classification\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print the model summary to inspect the architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_encodings(corpus, bert_tokenizer, max_len, trucation=True, padding=True):\n",
    "    return bert_tokenizer(corpus, max_length=max_len, truncation=trucation, padding=padding)\n",
    "    \n",
    "encodings = construct_encodings(corpus, bert_tokenizer, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in encodings: dict_keys(['input_ids', 'attention_mask'])\n",
      "Sample length of input_ids: 12854\n"
     ]
    }
   ],
   "source": [
    "# Check the structure and length\n",
    "print(\"Keys in encodings:\", encodings.keys())\n",
    "print(\"Sample length of input_ids:\", len(encodings['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tfdataset(encodings, y=None):\n",
    "    if y:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
    "    else:\n",
    "        # this case is used when making predictions on unseen samples after training\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    \n",
    "tfdataset = construct_tfdataset(encodings, y)\n",
    "\n",
    "TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_size = int(len(corpus) * (1-TEST_SPLIT))\n",
    "\n",
    "tfdataset = tfdataset.shuffle(len(corpus))\n",
    "tfdataset_train = tfdataset.take(train_size)\n",
    "tfdataset_test = tfdataset.skip(train_size)\n",
    "\n",
    "tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\n",
    "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import activations, optimizers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade transformers\n",
    "# !pip install tf-keras\n",
    "# import os\n",
    "# os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.optimizer_v2 import adam as adam_v2\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x00000152C19865C0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x00000152C19865C0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/sparse_categorical_crossentropy/Cast defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n\n  File \"C:\\Users\\bugat\\AppData\\Local\\Temp\\ipykernel_20684\\3602578007.py\", line 8, in <module>\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1170, in fit\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1804, in fit\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1381, in step_function\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1646, in train_step\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1647, in train_step\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\compile_utils.py\", line 269, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\compile_utils.py\", line 269, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 143, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 270, in call\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses\\loss.py\", line 38, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tree.py\", line 239, in map_structure\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\optree\\ops.py\", line 594, in tree_map\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses\\loss.py\", line 39, in <lambda>\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\core.py\", line 495, in convert_to_tensor\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py\", line 121, in convert_to_tensor\n\nCast string to float is not supported\n\t [[{{node sparse_categorical_crossentropy/sparse_categorical_crossentropy/Cast}}]] [Op:__inference_train_function_20094]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(tfdataset_train, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, epochs\u001b[38;5;241m=\u001b[39mN_EPOCHS)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1170\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1169\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node sparse_categorical_crossentropy/sparse_categorical_crossentropy/Cast defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n\n  File \"C:\\Users\\bugat\\AppData\\Local\\Temp\\ipykernel_20684\\3602578007.py\", line 8, in <module>\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1170, in fit\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1804, in fit\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1381, in step_function\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1646, in train_step\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1647, in train_step\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\compile_utils.py\", line 269, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\compile_utils.py\", line 269, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 143, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 270, in call\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses\\loss.py\", line 38, in __call__\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tree.py\", line 239, in map_structure\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\optree\\ops.py\", line 594, in tree_map\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses\\loss.py\", line 39, in <lambda>\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\core.py\", line 495, in convert_to_tensor\n\n  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py\", line 121, in convert_to_tensor\n\nCast string to float is not supported\n\t [[{{node sparse_categorical_crossentropy/sparse_categorical_crossentropy/Cast}}]] [Op:__inference_train_function_20094]"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "# optimizer = adam_v2(learning_rate=3e-5)\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
