{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.15.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM, Concatenate, Dropout, Dense, Layer, Multiply, TimeDistributed, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras_tuner import HyperModel, RandomSearch, BayesianOptimization, Hyperband\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22234 unique tokens.\n",
      "\n",
      "First 10 is listen below:\n",
      "{'<OOV>': 1, 'people': 2, 'like': 3, 'work': 4, 'right': 5, 'trump': 6, 'think': 7, 'state': 8, 'government': 9, 'party': 10}\n"
     ]
    }
   ],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "df = pd.read_csv(url_data + 'new_df.csv')\n",
    "\n",
    "df['All_text'] = df['All_text'].replace(['U.S.', 'U.S.A.'], ['US', 'USA'], regex=True)\n",
    "df['Processed'] = df['Processed'].fillna(0)\n",
    "df['Processed'] = df['Processed'].astype(str)\n",
    "df['All_text'] = df['All_text'].fillna(0)\n",
    "df['All_text'] = df['All_text'].astype(str)\n",
    "\n",
    "# df.to_csv('new_df.csv', index=False)\n",
    "\n",
    "# Making the relevant columns to lists\n",
    "all_texts = (df['All_text'].to_list())\n",
    "texts = df['Processed'].to_list()\n",
    "\n",
    "# Setting the wanted text for further modelling\n",
    "corpus = texts\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='<OOV>') # Hyperparameters = num_words=vocab_size, oov_token=oov_tok\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.\\n\\nFirst 10 is listen below:')\n",
    "print(dict(list(word_index.items())[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_train_LSTM = pd.read_csv(url_data+'X_tensorflow.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "X_train_LSTM = X_train_LSTM.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_train_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y_train_LSTM = y_train_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_LSTM, y_train_LSTM, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters with TensorBoard HParams API\n",
    "HP_FILTERS = hp.HParam('filters', hp.Discrete([32, 35]))\n",
    "HP_LSTM_UNITS = hp.HParam('lstm_units', hp.Discrete([64, 128, 152]))\n",
    "HP_LSTM_Dense = hp.HParam('lstm_dense_units', hp.Discrete([64, 128, 152]))\n",
    "HP_DENSE_UNITS = hp.HParam('dense_units', hp.Discrete([64, 128, 152]))\n",
    "HP_DROPOUT2 = hp.HParam('dropout2', hp.RealInterval(0.2, 0.6))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.001, 0.01))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "31                |31                |conv_filter_units\n",
      "115               |115               |lstm_dense_units\n",
      "101               |101               |lstm_units\n",
      "0.02              |0.02              |lstm_dropout_rate\n",
      "0.01              |0.01              |lstm_r_dropout_rate\n",
      "144               |144               |dense_units\n",
      "0.0083579         |0.0083579         |learning_rate\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TextClassifierHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, embeddings_GloVe, num_classes, parallel_blocks):#, include_attention_weights=False):\n",
    "        self.input_shape = input_shape\n",
    "        self.embeddings_GloVe = embeddings_GloVe\n",
    "        self.num_classes = num_classes\n",
    "        self.parallel_blocks = parallel_blocks\n",
    "        #self.include_attention_weights = include_attention_weights\n",
    "\n",
    "\n",
    "    def build(self, hp):\n",
    "        sequence_input = Input(shape=(self.input_shape,), dtype='int32')\n",
    "        embedded_sequences = Embedding(input_dim=self.embeddings_GloVe.shape[0],\n",
    "                                       output_dim=self.embeddings_GloVe.shape[1],\n",
    "                                       weights=[self.embeddings_GloVe],\n",
    "                                       trainable=False)(sequence_input)\n",
    "                                    #    embeddings_regularizer=keras.regularizers.l2(hp.Float('L2_rate', \n",
    "                                    #                              min_value=0.0, \n",
    "                                    #                              max_value=0.2, \n",
    "                                    #                             step=0.3))\n",
    "                                       \n",
    "            \n",
    "\n",
    "        conv_blocks = []\n",
    "        lstm_blocks = []\n",
    "\n",
    "        for _ in range (self.parallel_blocks): #(hp.Int('blocks', 1, self.parallel_blocks)):\n",
    "            conv = Conv1D(\n",
    "                filters=hp.Int('conv_filter_units', min_value=30, max_value=32, step=1),\n",
    "                kernel_size= 1, #hp.Int('conv_kernel_size', min_value=1, max_value=3, step=1),\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                strides=1)(embedded_sequences)\n",
    "                \n",
    "                                                                                            \n",
    "            # Apply a TimeDistributed Dense layer to each timestep of the convolutional block's output\n",
    "            conv_dense = TimeDistributed(Dense(hp.Int('lstm_dense_units', min_value=114, max_value=125, step=1), activation='relu'))(conv)\n",
    "            conv_blocks.append(conv_dense)\n",
    "\n",
    "            lstm = Bidirectional(LSTM(\n",
    "                units=hp.Int('lstm_units', min_value=100, max_value=110, step=1),\n",
    "                return_sequences=True,\n",
    "                dropout=hp.Float('lstm_dropout_rate', min_value=0.0, max_value=0.04, step=0.01),\n",
    "                recurrent_dropout=hp.Float('lstm_r_dropout_rate', min_value=0.01, max_value=0.06, step=0.01)\n",
    "            ))(conv_dense)  # Pass the output of the Dense layer into LSTM\n",
    "            lstm_blocks.append(lstm)\n",
    "\n",
    "        combined = Concatenate()(conv_blocks + lstm_blocks)\n",
    "        # Apply the custom attention mechanism\n",
    "        attention_layer = AdditiveAttention(use_scale=True)\n",
    "        attention_output = attention_layer([combined, combined], return_attention_scores=False)\n",
    "        context_vector = GlobalAveragePooling1D()(attention_output)\n",
    "\n",
    "        dense = Dense(units=hp.Int('dense_units', min_value=142, max_value=152, step=1), activation='relu')(context_vector)\n",
    "        dropout = Dropout(0.1)(dense) #Dropout(hp.Float('dropout_rate', min_value=0.0, max_value=0.2, step=0.1))\n",
    "        outdata = Dense(self.num_classes, activation='sigmoid')(dropout)\n",
    "        output = (outdata)\n",
    "        model = Model(inputs=sequence_input, outputs=output)\n",
    "        \n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "# Note: When actually using this model for training in tuner, make sure to remove attention_weights from outputs.\n",
    "# Hyperparameters and settings\n",
    "input_length = 20\n",
    "num_classes = 1\n",
    "parallel_blocks = 2\n",
    "log_dir = 'logs/fit/' + datetime.now().strftime(\"%d-%m-%Y %H-%M-%S\")\n",
    "\n",
    "# Use this for training (without attention weights in outputs)\n",
    "hypermodel_for_training = TextClassifierHyperModel(input_length, embeddings_GloVe, num_classes, parallel_blocks)#, include_attention_weights=False)\n",
    "\n",
    "\n",
    "# # Use this for visualization (with attention weights in outputs)\n",
    "# hypermodel_for_visualization = TextClassifierHyperModel(input_length, embeddings_GloVe, num_classes, parallel_blocks, include_attention_weights=True)\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    hypermodel_for_training,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Set the maximum number of trials (model configurations to test)\n",
    "    executions_per_trial=1,  # Number of models that should be built and fit for each trial\n",
    "    directory=log_dir,\n",
    "    project_name='TextClassification'\n",
    ")\n",
    "\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=0,  # No histogram computation, set to 1 or higher to compute histograms every '1' epoch or specified frequency\n",
    "    update_freq='epoch'  # Log metrics and histograms every epoch (default), not every batch\n",
    ")\n",
    "\n",
    "# Setup the checkpoint location\n",
    "checkpoint_filepath = '/models/model'\n",
    "\n",
    "# Create a ModelCheckpoint callback\n",
    "model_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "\n",
    "# Assume X_train, y_train, X_val, y_val are defined\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=5, \n",
    "             validation_data=(X_test, y_test),\n",
    "             callbacks=[tensorboard_callback, model_callback])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model\\assets\n"
     ]
    }
   ],
   "source": [
    "best_model.save(r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 20)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 20, 100)              2223500   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 20, 31)               3131      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 20, 31)               3131      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 20, 114)              3648      ['conv1d[0][0]']              \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 20, 114)              3648      ['conv1d_1[0][0]']            \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 20, 216)              192672    ['time_distributed[0][0]']    \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 20, 216)              192672    ['time_distributed_1[0][0]']  \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 20, 660)              0         ['time_distributed[0][0]',    \n",
      "                                                                     'time_distributed_1[0][0]',  \n",
      "                                                                     'bidirectional[0][0]',       \n",
      "                                                                     'bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " additive_attention (Additi  (None, 20, 660)              660       ['concatenate[0][0]',         \n",
      " veAttention)                                                        'concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 660)                  0         ['additive_attention[0][0]']  \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 150)                  99150     ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 150)                  0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1)                    151       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2722363 (10.38 MB)\n",
      "Trainable params: 498863 (1.90 MB)\n",
      "Non-trainable params: 2223500 (8.48 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits=5\n",
    "\n",
    "KF = KFold(n_splits=n_splits, shuffle=True, random_state=42) # Example: 5-fold cross-validation\n",
    "\n",
    "# Prepare arrays to store results for each fold\n",
    "fold_no = 1\n",
    "loss_per_fold = []\n",
    "acc_per_fold = []\n",
    "\n",
    "for train, test in KF.split(X_train_LSTM, y_train_LSTM):\n",
    "    # Create a fresh model for each fold\n",
    "\n",
    "    # Fit the model\n",
    "    best_model.fit(X_train_LSTM[train], y_train_LSTM[train],\n",
    "                        epochs=n_splits,  # Adjust based on your needs\n",
    "                        batch_size=64,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)],\n",
    "                        validation_data=(X_train_LSTM[test], y_train_LSTM[test]),\n",
    "                        verbose=1)  # You can set verbose to 0 to reduce logs\n",
    "\n",
    "    # Evaluate the model\n",
    "    scores = best_model.evaluate(X_train_LSTM[test], y_train_LSTM[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {best_model.metrics_names[0]} of {scores[0]}; {best_model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    loss_per_fold.append(scores[0])\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    fold_no += 1\n",
    "\n",
    "# Print average scores\n",
    "print(f'Average scores for all folds:\\n> Loss: {np.mean(loss_per_fold)}; Accuracy: {np.mean(acc_per_fold)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
